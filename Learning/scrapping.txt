It's true that data scientists often need to create their own datasets, especially when working on unique problems or when the required data isn't readily available in a structured format like a CSV file. This process is known as data collection or data scraping. Hereâ€™s an overview of the steps and tools involved:

Process of Making a Dataset from Information
Define the Problem and Data Requirements:

Understand the problem you are trying to solve and what kind of data you need.
Identify the sources where this data might be available (websites, APIs, databases, etc.).
Data Collection Methods:

Web Scraping: Extracting data from websites.
APIs: Using Application Programming Interfaces to pull data from services like Twitter, Google, or any service providing an API.
Databases: Querying databases to extract relevant information.
Manual Data Entry: Collecting data manually if automated methods are not feasible.
Data Cleaning and Preprocessing:

Clean the collected data to remove duplicates, handle missing values, and correct errors.
Preprocess the data to transform it into a suitable format for analysis.
Data Storage:

Store the cleaned data in a structured format like a CSV file, a database, or a data frame in Python.
Tools for Data Collection
Web Scraping:

Beautiful Soup: A Python library for parsing HTML and XML documents and extracting data.
Scrapy: An open-source and collaborative web crawling framework for Python.
Selenium: A browser automation tool that can be used for web scraping dynamic content.
APIs:

Requests: A Python library for making HTTP requests to interact with web APIs.
Tweepy: A Python library for accessing the Twitter API.
Google API Client: Libraries for accessing various Google APIs.
Databases:

SQL: Use SQL queries to extract data from relational databases like MySQL, PostgreSQL, or SQLite.
NoSQL: Tools like MongoDB for working with NoSQL databases.